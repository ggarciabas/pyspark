{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.0 64-bit ('venv')",
   "metadata": {
    "interpreter": {
     "hash": "aedda7876fc64012d0b0b47cd1c36c24442ba78edcbe99f4d61ad7e7487726aa"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('BFS-Superhero').getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The characters we wish to find the degree of separation between:\n",
    "startCharacterID = 5306 #SpiderMan\n",
    "targetCharacterID = 14  #ADAM 3,031 (who?)\n",
    "\n",
    "hitsCount = sc.accumulator(0)\n",
    "nodesEvaluated = sc.accumulator(0)\n",
    "exploredNodes = sc.accumulator(0)\n",
    "createdNodes = sc.accumulator(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COLOR: 1 - WHITE, 2 - GRAY, 3 - BLACK\n",
    "def node_connections_parse (line):\n",
    "    nodes = line.split()\n",
    "    hero = int(nodes[0])\n",
    "    connections = []\n",
    "    for node in nodes[1:]:\n",
    "        connections.append(int(node))\n",
    "\n",
    "    color = 1\n",
    "    distance = 9999\n",
    "    if hero == startCharacterID:\n",
    "        color = 2\n",
    "        distance = 0\n",
    "\n",
    "    return (hero, (connections, distance, color))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_connections (tuple1, tuple2):\n",
    "    \"\"\"\n",
    "    Agrupa conexões, no arquivo marvel graph podem existir diferentes linhas para cada hero\n",
    "    \"\"\"\n",
    "    connections = tuple1[0]\n",
    "    connections.extend(tuple2[0])\n",
    "    return (connections, tuple1[1], tuple1[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = sc.textFile('file:////Users/giovanna/Documents/GitHub/pyspark/SparkCourse/Marvel-graph.txt')\n",
    "graph = graph.map(node_connections_parse) # total size do not change!\n",
    "graph = graph.reduceByKey(lambda x,y: merge_connections(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bfsMap (node):\n",
    "    \"\"\"\n",
    "    Para o registro avaliado identifica se a cor é cinza (cor de exploração),\n",
    "        caso positivo cria para cada uma das conexoes um novo nó com a distancia\n",
    "        nova e a cor de exploracao.\n",
    "        Atualiza o registro atual com a nova cor (de já processado = BLACK).\n",
    "    \"\"\"\n",
    "    nodesEvaluated.add(1)\n",
    "    heroId = node[0]\n",
    "    data = node[1]\n",
    "    connections = data[0]\n",
    "    distance = data[1]\n",
    "    color = data[2]\n",
    "\n",
    "    newNodes = []\n",
    "\n",
    "    # GRAY mean to explore\n",
    "    if color == 2:\n",
    "        exploredNodes.add(1)\n",
    "        # criar novas conexoes\n",
    "        for connection in connections:\n",
    "            createdNodes.add(1)\n",
    "            newNodes.append((connection, ([], distance + 1, 2))) # (ID, (CONNECTIONS, DISTANCE, COLOR=GRAY))\n",
    "            if connection == targetCharacterID:\n",
    "                hitsCount.add(1) # soma o acumulador\n",
    "        color = 3\n",
    "    newNodes.append((heroId, (connections, distance, color))) # altera cor BLACK para o registro avaliado\n",
    "    return newNodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_rdd (tuple1, tuple2):\n",
    "    \"\"\"\n",
    "    Identifica códigos repetidos e mantém os dados de interesse.\n",
    "    Menor distancia, maior coloração e conexões.\n",
    "    \"\"\"\n",
    "    connections = tuple1[0] if len(tuple1[0]) > 0 else tuple2[0]\n",
    "    distance = tuple1[1] if tuple1[1] < tuple2[1] else tuple2[1]\n",
    "    color = tuple1[2] if tuple1[2] > tuple2[2] else tuple2[2]\n",
    "    return (connections, distance, color) # não necessita retornar chave, pois somente valores são passados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Iteração 0 total de nós 6486.\n",
      "Total de nós gerados 1741 após 6486 nós avaliados,  1 nós explorados e 1741 nós criados.\n",
      "Iteração 1 total de nós 6486.\n",
      "Total de nós gerados 214129 após 19458 nós avaliados,  1743 nós explorados e 217611 nós criados.\n",
      "Iteração 2 total de nós 6486.\n",
      "Total de nós gerados 120261 após 32430 nós avaliados,  8140 nós explorados e 552001 nós criados.\n",
      "Alvo encontrado com distância de 2, por 16 caminhos.\n"
     ]
    }
   ],
   "source": [
    "for it in range(0,10): # busca por até 10 saltos\n",
    "    total = graph.count()\n",
    "    print (f\"Iteração {it} total de nós {total}.\")\n",
    "    # processa nós de cor cinza\n",
    "    expanded_graph = graph.flatMap(bfsMap)\n",
    "    print (f\"Total de nós gerados {expanded_graph.count()-total} após {nodesEvaluated.value} nós avaliados,  {exploredNodes.value} nós explorados e {createdNodes} nós criados.\") # forca execução do flatMap para identificar se já encontrou o alvo\n",
    "\n",
    "    if hitsCount.value > 1:\n",
    "        print (f\"Alvo encontrado com distância de {it}, por {hitsCount.value} caminhos.\")\n",
    "        break\n",
    "\n",
    "    # limpa RDD, para que não se tenham chaves duplicadas\n",
    "    graph = expanded_graph.reduceByKey(clean_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}