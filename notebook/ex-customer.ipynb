{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "##Â Using RDD"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "sc = SparkContext(conf = SparkConf().setMaster('local').setAppName('Customer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sc.textFile('file:////Users/giovanna/Documents/GitHub/pyspark/SparkCourse/customer-orders.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse (line):\n",
    "    fields = line.split(',')\n",
    "    return (fields[0], float(fields[2]))\n",
    "rdd = lines.map(parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_spent = rdd.reduceByKey(lambda x,y: x[1]+y[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_spent.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cli in total_spent.collect():\n",
    "    print (str(cli[0]) + ' R$' + str(cli[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_spent.take(2)"
   ]
  },
  {
   "source": [
    "----"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Using DF"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('CustomerDF').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, FloatType\n",
    "schema = StructType([StructField(\"id\", IntegerType(), True),\\\n",
    "                     StructField('id_prod', IntegerType(), True),\\\n",
    "                     StructField('total', FloatType(), True)])\n",
    "sdf = spark.read.schema(schema).csv('file:////Users/giovanna/Documents/GitHub/pyspark/SparkCourse/customer-orders.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---+-------+-----+\n| id|id_prod|total|\n+---+-------+-----+\n| 44|   8602|37.19|\n| 35|   5368|65.89|\n+---+-------+-----+\nonly showing top 2 rows\n\n"
     ]
    }
   ],
   "source": [
    "sdf.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---+-----------------+\n| id|       sum(total)|\n+---+-----------------+\n| 31|4765.050008416176|\n| 85| 5503.42998456955|\n+---+-----------------+\nonly showing top 2 rows\n\n"
     ]
    }
   ],
   "source": [
    "total_spent = sdf.select('id', 'total').groupby('id').sum('total')\n",
    "total_spent.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---+-----------+\n| id|total_spent|\n+---+-----------+\n| 68|    6375.45|\n| 73|     6206.2|\n+---+-----------+\nonly showing top 2 rows\n\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "total_spent = sdf.select('id', 'total')\\\n",
    "                    .groupby('id')\\\n",
    "                    .agg(F.round(F.sum('total'),2).alias('total_spent'))\\\n",
    "                    .sort(\"total_spent\", ascending=False)\n",
    "total_spent.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}